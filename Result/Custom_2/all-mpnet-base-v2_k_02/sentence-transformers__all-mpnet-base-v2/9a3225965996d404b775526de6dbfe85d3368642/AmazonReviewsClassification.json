{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 66.93474698066711,
  "kg_co2_emissions": null,
  "mteb_version": "1.15.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.31338000000000005,
        "f1": 0.3073813963088326,
        "f1_weighted": 0.3073813963088326,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31338000000000005,
        "scores_per_experiment": [
          {
            "accuracy": 0.3286,
            "f1": 0.32495610926812785,
            "f1_weighted": 0.3249561092681279
          },
          {
            "accuracy": 0.3388,
            "f1": 0.332294219455605,
            "f1_weighted": 0.332294219455605
          },
          {
            "accuracy": 0.3018,
            "f1": 0.29382561941837954,
            "f1_weighted": 0.2938256194183796
          },
          {
            "accuracy": 0.3154,
            "f1": 0.31570461805463923,
            "f1_weighted": 0.3157046180546392
          },
          {
            "accuracy": 0.313,
            "f1": 0.3020030176764995,
            "f1_weighted": 0.3020030176764995
          },
          {
            "accuracy": 0.3184,
            "f1": 0.3081106108737062,
            "f1_weighted": 0.30811061087370617
          },
          {
            "accuracy": 0.2896,
            "f1": 0.2890632696590575,
            "f1_weighted": 0.2890632696590575
          },
          {
            "accuracy": 0.3168,
            "f1": 0.31528783924161374,
            "f1_weighted": 0.3152878392416137
          },
          {
            "accuracy": 0.2862,
            "f1": 0.27989765851410336,
            "f1_weighted": 0.27989765851410336
          },
          {
            "accuracy": 0.3252,
            "f1": 0.3126710009265945,
            "f1_weighted": 0.3126710009265945
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.30952,
        "f1": 0.3044906363727162,
        "f1_weighted": 0.30449063637271623,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30952,
        "scores_per_experiment": [
          {
            "accuracy": 0.326,
            "f1": 0.32560676342262074,
            "f1_weighted": 0.3256067634226208
          },
          {
            "accuracy": 0.342,
            "f1": 0.334411729480153,
            "f1_weighted": 0.334411729480153
          },
          {
            "accuracy": 0.3024,
            "f1": 0.29748726606415177,
            "f1_weighted": 0.2974872660641518
          },
          {
            "accuracy": 0.3028,
            "f1": 0.30288356988320136,
            "f1_weighted": 0.3028835698832013
          },
          {
            "accuracy": 0.3058,
            "f1": 0.29632427740989387,
            "f1_weighted": 0.29632427740989387
          },
          {
            "accuracy": 0.319,
            "f1": 0.3096324633980257,
            "f1_weighted": 0.30963246339802575
          },
          {
            "accuracy": 0.2898,
            "f1": 0.2908281435250721,
            "f1_weighted": 0.2908281435250721
          },
          {
            "accuracy": 0.3052,
            "f1": 0.3032867573539896,
            "f1_weighted": 0.3032867573539896
          },
          {
            "accuracy": 0.2942,
            "f1": 0.288882585095558,
            "f1_weighted": 0.288882585095558
          },
          {
            "accuracy": 0.308,
            "f1": 0.2955628080944962,
            "f1_weighted": 0.29556280809449625
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}