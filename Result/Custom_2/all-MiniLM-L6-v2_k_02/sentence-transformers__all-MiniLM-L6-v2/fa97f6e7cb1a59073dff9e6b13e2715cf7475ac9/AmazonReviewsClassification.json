{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 26.828162908554077,
  "kg_co2_emissions": null,
  "mteb_version": "1.15.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.31355999999999995,
        "f1": 0.3091838298228059,
        "f1_weighted": 0.3091838298228059,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31355999999999995,
        "scores_per_experiment": [
          {
            "accuracy": 0.324,
            "f1": 0.32312231727721247,
            "f1_weighted": 0.3231223172772125
          },
          {
            "accuracy": 0.3484,
            "f1": 0.3411159965221489,
            "f1_weighted": 0.3411159965221489
          },
          {
            "accuracy": 0.3236,
            "f1": 0.3180445227770236,
            "f1_weighted": 0.3180445227770236
          },
          {
            "accuracy": 0.308,
            "f1": 0.30957391685392566,
            "f1_weighted": 0.3095739168539256
          },
          {
            "accuracy": 0.3228,
            "f1": 0.3142093568622167,
            "f1_weighted": 0.3142093568622167
          },
          {
            "accuracy": 0.3234,
            "f1": 0.31846103751648797,
            "f1_weighted": 0.318461037516488
          },
          {
            "accuracy": 0.2824,
            "f1": 0.2823106747867858,
            "f1_weighted": 0.2823106747867858
          },
          {
            "accuracy": 0.3096,
            "f1": 0.3099823653191088,
            "f1_weighted": 0.3099823653191087
          },
          {
            "accuracy": 0.288,
            "f1": 0.2789188853518729,
            "f1_weighted": 0.2789188853518729
          },
          {
            "accuracy": 0.3054,
            "f1": 0.2960992249612761,
            "f1_weighted": 0.2960992249612761
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.30508,
        "f1": 0.3010823049433815,
        "f1_weighted": 0.3010823049433815,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30508,
        "scores_per_experiment": [
          {
            "accuracy": 0.3004,
            "f1": 0.30215353329156297,
            "f1_weighted": 0.302153533291563
          },
          {
            "accuracy": 0.342,
            "f1": 0.33481206166462474,
            "f1_weighted": 0.33481206166462474
          },
          {
            "accuracy": 0.3184,
            "f1": 0.3131627463441063,
            "f1_weighted": 0.31316274634410624
          },
          {
            "accuracy": 0.2902,
            "f1": 0.2917020515729728,
            "f1_weighted": 0.2917020515729728
          },
          {
            "accuracy": 0.3094,
            "f1": 0.3009835248124678,
            "f1_weighted": 0.3009835248124678
          },
          {
            "accuracy": 0.329,
            "f1": 0.32307592965625354,
            "f1_weighted": 0.32307592965625354
          },
          {
            "accuracy": 0.2788,
            "f1": 0.2790403473717268,
            "f1_weighted": 0.2790403473717267
          },
          {
            "accuracy": 0.2862,
            "f1": 0.2872684108493867,
            "f1_weighted": 0.2872684108493867
          },
          {
            "accuracy": 0.2906,
            "f1": 0.2812342406808999,
            "f1_weighted": 0.2812342406808999
          },
          {
            "accuracy": 0.3058,
            "f1": 0.29739020318981363,
            "f1_weighted": 0.29739020318981363
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}