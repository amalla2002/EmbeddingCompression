{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 27.855868577957153,
  "kg_co2_emissions": null,
  "mteb_version": "1.15.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.30101999999999995,
        "f1": 0.29588869055293665,
        "f1_weighted": 0.2958886905529366,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30101999999999995,
        "scores_per_experiment": [
          {
            "accuracy": 0.3128,
            "f1": 0.3089753073249161,
            "f1_weighted": 0.3089753073249161
          },
          {
            "accuracy": 0.3304,
            "f1": 0.323185522548321,
            "f1_weighted": 0.323185522548321
          },
          {
            "accuracy": 0.3164,
            "f1": 0.3089137124017875,
            "f1_weighted": 0.3089137124017875
          },
          {
            "accuracy": 0.2874,
            "f1": 0.28841902641656997,
            "f1_weighted": 0.28841902641656997
          },
          {
            "accuracy": 0.307,
            "f1": 0.29830882367665257,
            "f1_weighted": 0.2983088236766525
          },
          {
            "accuracy": 0.3166,
            "f1": 0.3098717424896681,
            "f1_weighted": 0.3098717424896681
          },
          {
            "accuracy": 0.2686,
            "f1": 0.26762407336530847,
            "f1_weighted": 0.2676240733653084
          },
          {
            "accuracy": 0.2936,
            "f1": 0.29426437037576514,
            "f1_weighted": 0.29426437037576514
          },
          {
            "accuracy": 0.2814,
            "f1": 0.2723609913292029,
            "f1_weighted": 0.27236099132920283
          },
          {
            "accuracy": 0.296,
            "f1": 0.28696333560117476,
            "f1_weighted": 0.2869633356011747
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.29688,
        "f1": 0.29266902002628775,
        "f1_weighted": 0.29266902002628775,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.29688,
        "scores_per_experiment": [
          {
            "accuracy": 0.2996,
            "f1": 0.29968541189739695,
            "f1_weighted": 0.2996854118973969
          },
          {
            "accuracy": 0.3316,
            "f1": 0.3244554057454948,
            "f1_weighted": 0.32445540574549486
          },
          {
            "accuracy": 0.3166,
            "f1": 0.3102502871432727,
            "f1_weighted": 0.3102502871432727
          },
          {
            "accuracy": 0.2826,
            "f1": 0.28434782326211605,
            "f1_weighted": 0.28434782326211605
          },
          {
            "accuracy": 0.3004,
            "f1": 0.2910476515945299,
            "f1_weighted": 0.2910476515945299
          },
          {
            "accuracy": 0.3208,
            "f1": 0.31416337111956844,
            "f1_weighted": 0.31416337111956844
          },
          {
            "accuracy": 0.2624,
            "f1": 0.26218751735949464,
            "f1_weighted": 0.2621875173594945
          },
          {
            "accuracy": 0.2802,
            "f1": 0.2816858462463682,
            "f1_weighted": 0.2816858462463682
          },
          {
            "accuracy": 0.2846,
            "f1": 0.276132779317524,
            "f1_weighted": 0.276132779317524
          },
          {
            "accuracy": 0.29,
            "f1": 0.282734106577112,
            "f1_weighted": 0.28273410657711207
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}