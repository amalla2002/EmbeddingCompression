{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 182.64974761009216,
  "kg_co2_emissions": null,
  "mteb_version": "1.15.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.51692,
        "f1": 0.49853016607106176,
        "f1_weighted": 0.49853016607106176,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.51692,
        "scores_per_experiment": [
          {
            "accuracy": 0.5202,
            "f1": 0.49942429609360267,
            "f1_weighted": 0.49942429609360267
          },
          {
            "accuracy": 0.5052,
            "f1": 0.4828035958191147,
            "f1_weighted": 0.48280359581911464
          },
          {
            "accuracy": 0.5196,
            "f1": 0.5038100547946781,
            "f1_weighted": 0.5038100547946781
          },
          {
            "accuracy": 0.53,
            "f1": 0.5210756168652648,
            "f1_weighted": 0.5210756168652648
          },
          {
            "accuracy": 0.5212,
            "f1": 0.490255840495073,
            "f1_weighted": 0.49025584049507304
          },
          {
            "accuracy": 0.5358,
            "f1": 0.5224704427046768,
            "f1_weighted": 0.5224704427046768
          },
          {
            "accuracy": 0.5006,
            "f1": 0.49668161147640744,
            "f1_weighted": 0.4966816114764075
          },
          {
            "accuracy": 0.5388,
            "f1": 0.5296949983904702,
            "f1_weighted": 0.5296949983904701
          },
          {
            "accuracy": 0.5098,
            "f1": 0.4810374842847748,
            "f1_weighted": 0.48103748428477466
          },
          {
            "accuracy": 0.488,
            "f1": 0.45804771978655545,
            "f1_weighted": 0.45804771978655545
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5118199999999999,
        "f1": 0.4939168016913933,
        "f1_weighted": 0.4939168016913933,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5118199999999999,
        "scores_per_experiment": [
          {
            "accuracy": 0.5112,
            "f1": 0.49393449938167305,
            "f1_weighted": 0.49393449938167305
          },
          {
            "accuracy": 0.5014,
            "f1": 0.4787172548788098,
            "f1_weighted": 0.4787172548788097
          },
          {
            "accuracy": 0.5148,
            "f1": 0.5010062773367056,
            "f1_weighted": 0.5010062773367056
          },
          {
            "accuracy": 0.5246,
            "f1": 0.5166227871348862,
            "f1_weighted": 0.5166227871348861
          },
          {
            "accuracy": 0.5106,
            "f1": 0.4759105938625396,
            "f1_weighted": 0.47591059386253964
          },
          {
            "accuracy": 0.5258,
            "f1": 0.5127953231703166,
            "f1_weighted": 0.5127953231703166
          },
          {
            "accuracy": 0.4944,
            "f1": 0.48835186040338324,
            "f1_weighted": 0.4883518604033833
          },
          {
            "accuracy": 0.5344,
            "f1": 0.5262218750823914,
            "f1_weighted": 0.5262218750823914
          },
          {
            "accuracy": 0.5108,
            "f1": 0.48376071715654695,
            "f1_weighted": 0.483760717156547
          },
          {
            "accuracy": 0.4902,
            "f1": 0.4618468285066806,
            "f1_weighted": 0.4618468285066806
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}