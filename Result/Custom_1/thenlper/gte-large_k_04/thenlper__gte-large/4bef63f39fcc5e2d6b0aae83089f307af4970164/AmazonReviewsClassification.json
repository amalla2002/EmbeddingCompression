{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 180.15174055099487,
  "kg_co2_emissions": null,
  "mteb_version": "1.15.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.5147,
        "f1": 0.4980515429547854,
        "f1_weighted": 0.4980515429547854,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5147,
        "scores_per_experiment": [
          {
            "accuracy": 0.5214,
            "f1": 0.5022477188352299,
            "f1_weighted": 0.5022477188352301
          },
          {
            "accuracy": 0.5038,
            "f1": 0.4826219645783986,
            "f1_weighted": 0.4826219645783986
          },
          {
            "accuracy": 0.517,
            "f1": 0.5042484108044624,
            "f1_weighted": 0.5042484108044624
          },
          {
            "accuracy": 0.5286,
            "f1": 0.5220126084369506,
            "f1_weighted": 0.5220126084369505
          },
          {
            "accuracy": 0.5146,
            "f1": 0.48482323663792337,
            "f1_weighted": 0.4848232366379234
          },
          {
            "accuracy": 0.5302,
            "f1": 0.5180457994682914,
            "f1_weighted": 0.5180457994682915
          },
          {
            "accuracy": 0.498,
            "f1": 0.4917537802611872,
            "f1_weighted": 0.4917537802611872
          },
          {
            "accuracy": 0.5398,
            "f1": 0.5325402713277558,
            "f1_weighted": 0.5325402713277557
          },
          {
            "accuracy": 0.5094,
            "f1": 0.48789807292027676,
            "f1_weighted": 0.4878980729202767
          },
          {
            "accuracy": 0.4842,
            "f1": 0.4543235662773781,
            "f1_weighted": 0.4543235662773781
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.51198,
        "f1": 0.4956846320767351,
        "f1_weighted": 0.4956846320767351,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.51198,
        "scores_per_experiment": [
          {
            "accuracy": 0.5152,
            "f1": 0.5003325205563028,
            "f1_weighted": 0.5003325205563028
          },
          {
            "accuracy": 0.5018,
            "f1": 0.4810704860284747,
            "f1_weighted": 0.48107048602847463
          },
          {
            "accuracy": 0.51,
            "f1": 0.49990353259468245,
            "f1_weighted": 0.49990353259468245
          },
          {
            "accuracy": 0.5282,
            "f1": 0.5223401792180598,
            "f1_weighted": 0.5223401792180598
          },
          {
            "accuracy": 0.5106,
            "f1": 0.4768909569445797,
            "f1_weighted": 0.47689095694457967
          },
          {
            "accuracy": 0.5288,
            "f1": 0.516526361890023,
            "f1_weighted": 0.516526361890023
          },
          {
            "accuracy": 0.4928,
            "f1": 0.48528663388525384,
            "f1_weighted": 0.48528663388525384
          },
          {
            "accuracy": 0.5306,
            "f1": 0.523789844352215,
            "f1_weighted": 0.523789844352215
          },
          {
            "accuracy": 0.5154,
            "f1": 0.4931450933728739,
            "f1_weighted": 0.4931450933728738
          },
          {
            "accuracy": 0.4864,
            "f1": 0.45756071192488623,
            "f1_weighted": 0.4575607119248862
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}