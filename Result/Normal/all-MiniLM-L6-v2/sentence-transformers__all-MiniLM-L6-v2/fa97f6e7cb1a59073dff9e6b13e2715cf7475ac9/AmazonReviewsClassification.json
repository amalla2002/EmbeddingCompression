{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 10.434599161148071,
  "kg_co2_emissions": null,
  "mteb_version": "1.15.2",
  "scores": {
    "test": [
      {
        "accuracy": 0.30848000000000003,
        "f1": 0.3043009944052308,
        "f1_weighted": 0.3043009944052308,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30848000000000003,
        "scores_per_experiment": [
          {
            "accuracy": 0.3188,
            "f1": 0.31667615225009815,
            "f1_weighted": 0.31667615225009815
          },
          {
            "accuracy": 0.3452,
            "f1": 0.3381083276563167,
            "f1_weighted": 0.33810832765631665
          },
          {
            "accuracy": 0.3154,
            "f1": 0.3103299141735,
            "f1_weighted": 0.3103299141734999
          },
          {
            "accuracy": 0.3008,
            "f1": 0.3020317026310225,
            "f1_weighted": 0.3020317026310226
          },
          {
            "accuracy": 0.3172,
            "f1": 0.31006471249843653,
            "f1_weighted": 0.31006471249843653
          },
          {
            "accuracy": 0.3256,
            "f1": 0.32101902557251594,
            "f1_weighted": 0.321019025572516
          },
          {
            "accuracy": 0.2698,
            "f1": 0.2699015487426243,
            "f1_weighted": 0.26990154874262423
          },
          {
            "accuracy": 0.3052,
            "f1": 0.30523948501111126,
            "f1_weighted": 0.30523948501111126
          },
          {
            "accuracy": 0.2848,
            "f1": 0.27696982332410247,
            "f1_weighted": 0.2769698233241024
          },
          {
            "accuracy": 0.302,
            "f1": 0.2926692521925801,
            "f1_weighted": 0.2926692521925801
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.30302,
        "f1": 0.29983005188434414,
        "f1_weighted": 0.29983005188434414,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.30302,
        "scores_per_experiment": [
          {
            "accuracy": 0.3026,
            "f1": 0.3048554163301936,
            "f1_weighted": 0.3048554163301936
          },
          {
            "accuracy": 0.3446,
            "f1": 0.3375582560753593,
            "f1_weighted": 0.33755825607535933
          },
          {
            "accuracy": 0.3158,
            "f1": 0.3118627850751892,
            "f1_weighted": 0.3118627850751892
          },
          {
            "accuracy": 0.2898,
            "f1": 0.2916711793152056,
            "f1_weighted": 0.2916711793152056
          },
          {
            "accuracy": 0.305,
            "f1": 0.29810278978658855,
            "f1_weighted": 0.2981027897865885
          },
          {
            "accuracy": 0.3268,
            "f1": 0.32216075320423077,
            "f1_weighted": 0.32216075320423077
          },
          {
            "accuracy": 0.27,
            "f1": 0.27031801411699835,
            "f1_weighted": 0.2703180141169983
          },
          {
            "accuracy": 0.279,
            "f1": 0.28003789881478086,
            "f1_weighted": 0.28003789881478086
          },
          {
            "accuracy": 0.2914,
            "f1": 0.28499255910616766,
            "f1_weighted": 0.2849925591061677
          },
          {
            "accuracy": 0.3052,
            "f1": 0.2967408670187275,
            "f1_weighted": 0.2967408670187275
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}